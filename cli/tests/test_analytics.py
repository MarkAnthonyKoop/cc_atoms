#!/usr/bin/env python3
"""
Comprehensive tests for the CC Analytics Suite.

Tests are organized to test backend logic independently of GUI components.
"""

import json
import pytest
import tempfile
from collections import Counter
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any
from unittest.mock import Mock, patch, MagicMock


# ============================================================================
# Test Fixtures - Create mock session data
# ============================================================================

@pytest.fixture
def temp_projects_dir():
    """Create a temporary projects directory with mock session data."""
    with tempfile.TemporaryDirectory() as tmpdir:
        projects_dir = Path(tmpdir)

        # Create mock project directories
        project1 = projects_dir / "project-myapp"
        project1.mkdir()

        project2 = projects_dir / "project-api"
        project2.mkdir()

        yield projects_dir


@pytest.fixture
def sample_session_entries() -> List[Dict[str, Any]]:
    """Generate sample session entries in Claude Code format."""
    base_time = datetime(2025, 12, 10, 14, 30, 0)

    entries = [
        # User message
        {
            "type": "user",
            "uuid": "user-1",
            "sessionId": "session-123",
            "timestamp": base_time.isoformat(),
            "message": {
                "role": "user",
                "content": "Help me fix this bug"
            }
        },
        # Assistant with text
        {
            "type": "assistant",
            "uuid": "asst-1",
            "parentUuid": "user-1",
            "sessionId": "session-123",
            "timestamp": (base_time + timedelta(seconds=5)).isoformat(),
            "message": {
                "role": "assistant",
                "content": [
                    {"type": "text", "text": "I'll help you fix that bug."}
                ]
            }
        },
        # Assistant with tool use
        {
            "type": "assistant",
            "uuid": "asst-2",
            "parentUuid": "asst-1",
            "sessionId": "session-123",
            "timestamp": (base_time + timedelta(seconds=10)).isoformat(),
            "message": {
                "role": "assistant",
                "content": [
                    {
                        "type": "tool_use",
                        "id": "tool-1",
                        "name": "Read",
                        "input": {"file_path": "/path/to/file.py"}
                    }
                ]
            }
        },
        # User with tool result
        {
            "type": "user",
            "uuid": "user-2",
            "parentUuid": "asst-2",
            "sessionId": "session-123",
            "timestamp": (base_time + timedelta(seconds=15)).isoformat(),
            "message": {
                "role": "user",
                "content": [
                    {
                        "type": "tool_result",
                        "tool_use_id": "tool-1",
                        "content": "file contents here"
                    }
                ]
            }
        },
        # Assistant with Bash tool
        {
            "type": "assistant",
            "uuid": "asst-3",
            "sessionId": "session-123",
            "timestamp": (base_time + timedelta(seconds=20)).isoformat(),
            "message": {
                "role": "assistant",
                "content": [
                    {
                        "type": "tool_use",
                        "id": "tool-2",
                        "name": "Bash",
                        "input": {"command": "python3 test.py"}
                    }
                ]
            }
        },
        # User with error tool result
        {
            "type": "user",
            "uuid": "user-3",
            "sessionId": "session-123",
            "timestamp": (base_time + timedelta(seconds=25)).isoformat(),
            "message": {
                "role": "user",
                "content": [
                    {
                        "type": "tool_result",
                        "tool_use_id": "tool-2",
                        "is_error": True,
                        "content": "Error: file not found"
                    }
                ]
            }
        },
        # Assistant with Edit tool
        {
            "type": "assistant",
            "uuid": "asst-4",
            "sessionId": "session-123",
            "timestamp": (base_time + timedelta(seconds=30)).isoformat(),
            "message": {
                "role": "assistant",
                "content": [
                    {
                        "type": "tool_use",
                        "id": "tool-3",
                        "name": "Edit",
                        "input": {"file_path": "/path/to/file.py", "old_string": "foo", "new_string": "bar"}
                    }
                ]
            }
        },
    ]

    return entries


@pytest.fixture
def session_file(temp_projects_dir, sample_session_entries):
    """Create a session file with sample entries."""
    project_dir = temp_projects_dir / "project-myapp"
    session_file = project_dir / "session-123.jsonl"

    with open(session_file, 'w') as f:
        for entry in sample_session_entries:
            f.write(json.dumps(entry) + '\n')

    return session_file


# ============================================================================
# Tests for ConversationAnalyzer
# ============================================================================

class TestConversationAnalyzer:
    """Tests for the conversation analyzer backend."""

    def test_session_stats_dataclass(self):
        """Test SessionStats dataclass initialization."""
        from cc.tools.conversation_analyzer import SessionStats

        stats = SessionStats(session_id="test-123")
        assert stats.session_id == "test-123"
        assert stats.message_count == 0
        assert stats.tool_uses == 0
        assert stats.errors == 0
        assert isinstance(stats.tools_used, Counter)

    def test_session_stats_duration(self):
        """Test duration calculation."""
        from cc.tools.conversation_analyzer import SessionStats

        stats = SessionStats(session_id="test")
        stats.start_time = datetime(2025, 1, 1, 10, 0, 0)
        stats.end_time = datetime(2025, 1, 1, 10, 30, 0)

        assert stats.duration_minutes == 30.0

    def test_session_stats_duration_no_times(self):
        """Test duration returns 0 when times not set."""
        from cc.tools.conversation_analyzer import SessionStats

        stats = SessionStats(session_id="test")
        assert stats.duration_minutes == 0

    def test_aggregate_stats_dataclass(self):
        """Test AggregateStats dataclass initialization."""
        from cc.tools.conversation_analyzer import AggregateStats

        agg = AggregateStats()
        assert agg.total_sessions == 0
        assert isinstance(agg.tool_frequency, Counter)
        assert isinstance(agg.file_patterns, Counter)

    def test_parse_timestamp_iso(self):
        """Test timestamp parsing with ISO format."""
        from cc.tools.conversation_analyzer import ConversationAnalyzer

        analyzer = ConversationAnalyzer(Path("/tmp"))

        # Standard ISO
        ts = analyzer.parse_timestamp("2025-12-10T14:30:00")
        assert ts is not None
        assert ts.hour == 14
        assert ts.minute == 30

        # With Z suffix
        ts = analyzer.parse_timestamp("2025-12-10T14:30:00Z")
        assert ts is not None

        # With microseconds
        ts = analyzer.parse_timestamp("2025-12-10T14:30:00.123456")
        assert ts is not None

    def test_parse_timestamp_invalid(self):
        """Test timestamp parsing with invalid input."""
        from cc.tools.conversation_analyzer import ConversationAnalyzer

        analyzer = ConversationAnalyzer(Path("/tmp"))

        assert analyzer.parse_timestamp("") is None
        assert analyzer.parse_timestamp(None) is None
        assert analyzer.parse_timestamp("invalid") is None

    def test_analyze_session(self, temp_projects_dir, session_file):
        """Test analyzing a single session file."""
        from cc.tools.conversation_analyzer import ConversationAnalyzer

        analyzer = ConversationAnalyzer(temp_projects_dir / "project-myapp")
        stats = analyzer.analyze_session(session_file)

        assert stats is not None
        assert stats.session_id == "session-123"
        assert stats.user_messages >= 3  # 3 user messages
        assert stats.assistant_messages >= 3  # 3 assistant messages (with tool uses)
        assert stats.tool_uses >= 3  # Read, Bash, Edit
        assert stats.errors >= 1  # One error from Bash

    def test_analyze_session_tool_tracking(self, temp_projects_dir, session_file):
        """Test that tools are properly tracked."""
        from cc.tools.conversation_analyzer import ConversationAnalyzer

        analyzer = ConversationAnalyzer(temp_projects_dir / "project-myapp")
        stats = analyzer.analyze_session(session_file)

        assert "Read" in stats.tools_used
        assert "Bash" in stats.tools_used
        assert "Edit" in stats.tools_used

    def test_analyze_session_file_patterns(self, temp_projects_dir, session_file):
        """Test that file patterns are tracked."""
        from cc.tools.conversation_analyzer import ConversationAnalyzer

        analyzer = ConversationAnalyzer(temp_projects_dir / "project-myapp")
        analyzer.analyze_session(session_file)

        # Should track .py extension from Read and Edit
        assert ".py" in analyzer.aggregate.file_patterns

    def test_analyze_session_command_patterns(self, temp_projects_dir, session_file):
        """Test that bash commands are tracked."""
        from cc.tools.conversation_analyzer import ConversationAnalyzer

        analyzer = ConversationAnalyzer(temp_projects_dir / "project-myapp")
        analyzer.analyze_session(session_file)

        # Should track python3 command
        assert "python3" in analyzer.aggregate.command_patterns

    def test_analyze_nonexistent_file(self, temp_projects_dir):
        """Test handling of nonexistent files."""
        from cc.tools.conversation_analyzer import ConversationAnalyzer

        analyzer = ConversationAnalyzer(temp_projects_dir)
        stats = analyzer.analyze_session(temp_projects_dir / "nonexistent.jsonl")

        assert stats is None

    def test_analyze_empty_file(self, temp_projects_dir):
        """Test handling of empty files."""
        from cc.tools.conversation_analyzer import ConversationAnalyzer

        empty_file = temp_projects_dir / "project-myapp" / "empty.jsonl"
        empty_file.parent.mkdir(exist_ok=True)
        empty_file.touch()

        analyzer = ConversationAnalyzer(temp_projects_dir / "project-myapp")
        stats = analyzer.analyze_session(empty_file)

        assert stats is None

    def test_analyze_all(self, temp_projects_dir, session_file):
        """Test analyzing all sessions in a directory."""
        from cc.tools.conversation_analyzer import ConversationAnalyzer

        analyzer = ConversationAnalyzer(temp_projects_dir / "project-myapp")
        analyzer.analyze_all()

        assert len(analyzer.sessions) >= 1
        assert analyzer.aggregate.total_sessions >= 1

    def test_get_top_tools(self, temp_projects_dir, session_file):
        """Test getting top tools."""
        from cc.tools.conversation_analyzer import ConversationAnalyzer

        analyzer = ConversationAnalyzer(temp_projects_dir / "project-myapp")
        analyzer.analyze_all()

        top_tools = analyzer.get_top_tools(5)
        assert isinstance(top_tools, list)
        # Each item should be (tool_name, count) tuple
        for item in top_tools:
            assert len(item) == 2

    def test_get_productivity_metrics(self, temp_projects_dir, session_file):
        """Test productivity metrics calculation."""
        from cc.tools.conversation_analyzer import ConversationAnalyzer

        analyzer = ConversationAnalyzer(temp_projects_dir / "project-myapp")
        analyzer.analyze_all()

        metrics = analyzer.get_productivity_metrics()

        assert 'total_sessions' in metrics
        assert 'total_messages' in metrics
        assert 'total_tool_uses' in metrics
        assert 'error_rate' in metrics

    def test_generate_report(self, temp_projects_dir, session_file):
        """Test report generation."""
        from cc.tools.conversation_analyzer import ConversationAnalyzer

        analyzer = ConversationAnalyzer(temp_projects_dir / "project-myapp")
        analyzer.analyze_all()

        report = analyzer.generate_report()

        assert isinstance(report, str)
        assert "OVERVIEW" in report
        assert "TOP TOOLS" in report


# ============================================================================
# Tests for DashboardAnalyzer (backend only)
# ============================================================================

class TestDashboardAnalyzer:
    """Tests for the dashboard analyzer backend."""

    def test_session_data_dataclass(self):
        """Test SessionData dataclass."""
        from cc.tools.dashboard import SessionData

        data = SessionData()
        assert data.total_sessions == 0
        assert isinstance(data.tool_frequency, Counter)
        assert isinstance(data.recent_sessions, list)

    def test_analyzer_init(self, temp_projects_dir):
        """Test analyzer initialization."""
        from cc.tools.dashboard import DashboardAnalyzer

        analyzer = DashboardAnalyzer(temp_projects_dir)
        assert analyzer.projects_dir == temp_projects_dir
        assert analyzer.last_scan is None

    def test_parse_timestamp(self, temp_projects_dir):
        """Test timestamp parsing."""
        from cc.tools.dashboard import DashboardAnalyzer

        analyzer = DashboardAnalyzer(temp_projects_dir)

        ts = analyzer.parse_timestamp("2025-12-10T14:30:00Z")
        assert ts is not None

        ts = analyzer.parse_timestamp("")
        assert ts is None

    def test_scan_sessions(self, temp_projects_dir, session_file):
        """Test session scanning."""
        from cc.tools.dashboard import DashboardAnalyzer

        analyzer = DashboardAnalyzer(temp_projects_dir)
        data = analyzer.scan_sessions(max_files=100)

        assert data.total_sessions >= 1
        assert data.total_tool_uses >= 1
        assert analyzer.last_scan is not None

    def test_scan_sessions_max_files(self, temp_projects_dir, session_file):
        """Test max_files limit."""
        from cc.tools.dashboard import DashboardAnalyzer

        analyzer = DashboardAnalyzer(temp_projects_dir)
        data = analyzer.scan_sessions(max_files=0)

        # Should scan nothing with max_files=0
        assert data.total_sessions == 0

    def test_project_tracking(self, temp_projects_dir, session_file):
        """Test project name extraction and tracking."""
        from cc.tools.dashboard import DashboardAnalyzer

        analyzer = DashboardAnalyzer(temp_projects_dir)
        data = analyzer.scan_sessions()

        # Should track the project
        assert len(data.projects) >= 1


# ============================================================================
# Tests for ProductivityCoach (backend only)
# ============================================================================

class TestProductivityCoach:
    """Tests for the productivity coach backend."""

    def test_productivity_metrics_dataclass(self):
        """Test ProductivityMetrics dataclass."""
        from cc.tools.productivity_coach import ProductivityMetrics

        metrics = ProductivityMetrics()
        assert metrics.total_sessions == 0
        assert metrics.productivity_score == 0
        assert metrics.error_rate == 0

    def test_insight_dataclass(self):
        """Test Insight dataclass."""
        from cc.tools.productivity_coach import Insight

        insight = Insight(
            category="efficiency",
            title="Test Insight",
            description="Test description",
            recommendation="Test recommendation",
            priority=3,
            icon="ðŸ”§"
        )

        assert insight.category == "efficiency"
        assert insight.priority == 3

    def test_coach_init(self, temp_projects_dir):
        """Test coach initialization."""
        from cc.tools.productivity_coach import ProductivityCoach

        coach = ProductivityCoach(temp_projects_dir)
        assert coach.projects_dir == temp_projects_dir
        assert coach.total_sessions == 0

    def test_analyze_sessions(self, temp_projects_dir, session_file):
        """Test session analysis."""
        from cc.tools.productivity_coach import ProductivityCoach

        coach = ProductivityCoach(temp_projects_dir)
        coach.analyze_sessions(max_files=100)

        assert coach.total_sessions >= 1
        assert coach.total_tool_uses >= 1

    def test_compute_metrics(self, temp_projects_dir, session_file):
        """Test metrics computation."""
        from cc.tools.productivity_coach import ProductivityCoach

        coach = ProductivityCoach(temp_projects_dir)
        coach.analyze_sessions()

        metrics = coach.compute_metrics()

        assert metrics.total_sessions >= 1
        assert 0 <= metrics.efficiency_score <= 100
        assert 0 <= metrics.productivity_score <= 100

    def test_compute_metrics_error_rate(self, temp_projects_dir, session_file):
        """Test error rate calculation."""
        from cc.tools.productivity_coach import ProductivityCoach

        coach = ProductivityCoach(temp_projects_dir)
        coach.analyze_sessions()

        metrics = coach.compute_metrics()

        # Error rate should be calculable (may be 0 or positive)
        assert metrics.error_rate >= 0

    def test_generate_insights_high_error(self, temp_projects_dir):
        """Test insight generation for high error rate."""
        from cc.tools.productivity_coach import ProductivityCoach, ProductivityMetrics

        coach = ProductivityCoach(temp_projects_dir)

        # Create metrics with high error rate
        metrics = ProductivityMetrics()
        metrics.error_rate = 15.0  # 15% error rate
        metrics.avg_session_length_min = 30
        metrics.bash_percentage = 30
        metrics.read_edit_ratio = 2.0

        insights = coach.generate_insights(metrics)

        # Should have an insight about high error rate
        error_insights = [i for i in insights if "Error Rate" in i.title]
        assert len(error_insights) >= 1

    def test_generate_insights_long_sessions(self, temp_projects_dir):
        """Test insight generation for long sessions."""
        from cc.tools.productivity_coach import ProductivityCoach, ProductivityMetrics

        coach = ProductivityCoach(temp_projects_dir)

        metrics = ProductivityMetrics()
        metrics.error_rate = 2.0
        metrics.avg_session_length_min = 180  # 3 hours

        insights = coach.generate_insights(metrics)

        # Should have an insight about long sessions
        session_insights = [i for i in insights if "Long Sessions" in i.title]
        assert len(session_insights) >= 1

    def test_generate_insights_night_owl(self, temp_projects_dir):
        """Test insight generation for night owl pattern."""
        from cc.tools.productivity_coach import ProductivityCoach, ProductivityMetrics

        coach = ProductivityCoach(temp_projects_dir)

        metrics = ProductivityMetrics()
        metrics.peak_hour = 2  # 2 AM

        insights = coach.generate_insights(metrics)

        # Should have night owl insight
        night_insights = [i for i in insights if "Night Owl" in i.title or "ðŸŒ™" in i.icon]
        assert len(night_insights) >= 1

    def test_format_report(self, temp_projects_dir, session_file):
        """Test report formatting."""
        from cc.tools.productivity_coach import ProductivityCoach

        coach = ProductivityCoach(temp_projects_dir)
        coach.analyze_sessions()

        metrics = coach.compute_metrics()
        insights = coach.generate_insights(metrics)
        report = coach.format_report(metrics, insights)

        assert isinstance(report, str)
        assert "PRODUCTIVITY COACH" in report
        assert "YOUR SCORES" in report
        assert "KEY METRICS" in report

    def test_make_bar(self, temp_projects_dir):
        """Test progress bar creation."""
        from cc.tools.productivity_coach import ProductivityCoach

        coach = ProductivityCoach(temp_projects_dir)

        bar = coach._make_bar(50, width=20)
        assert "â–ˆ" in bar
        assert "â–‘" in bar
        assert len(bar) == 22  # [bar]

    def test_make_bar_edge_cases(self, temp_projects_dir):
        """Test progress bar edge cases."""
        from cc.tools.productivity_coach import ProductivityCoach

        coach = ProductivityCoach(temp_projects_dir)

        # 0%
        bar = coach._make_bar(0, width=10)
        assert bar.count("â–ˆ") == 0

        # 100%
        bar = coach._make_bar(100, width=10)
        assert bar.count("â–ˆ") == 10


# ============================================================================
# Tests for LiveMonitor (backend only - no actual file watching)
# ============================================================================

class TestLiveMonitor:
    """Tests for live monitor backend components."""

    def test_live_event_dataclass(self):
        """Test LiveEvent dataclass."""
        from cc.tools.live_monitor import LiveEvent

        event = LiveEvent(
            timestamp=datetime.now(),
            session_id="test-123",
            project="myapp",
            event_type="tool_use",
            content="Running command",
            tool_name="Bash",
            is_error=False
        )

        assert event.session_id == "test-123"
        assert event.tool_name == "Bash"
        assert event.is_error is False

    def test_colors_class(self):
        """Test Colors class has expected values."""
        from cc.tools.live_monitor import Colors

        assert Colors.RESET == '\033[0m'
        assert Colors.RED == '\033[91m'
        assert Colors.GREEN == '\033[92m'

    def test_session_watcher_init(self):
        """Test SessionWatcher initialization."""
        from cc.tools.live_monitor import SessionWatcher

        callback = Mock()
        watcher = SessionWatcher(callback)

        assert watcher.callback == callback
        assert len(watcher.file_positions) == 0
        assert len(watcher.known_files) == 0

    def test_session_watcher_ignores_non_jsonl(self):
        """Test that watcher ignores non-jsonl files."""
        from cc.tools.live_monitor import SessionWatcher

        callback = Mock()
        watcher = SessionWatcher(callback)

        # Create mock event for non-jsonl file
        event = Mock()
        event.is_directory = False
        event.src_path = "/path/to/file.txt"

        watcher.on_modified(event)

        # Callback should not be called
        callback.assert_not_called()

    def test_session_watcher_ignores_directories(self):
        """Test that watcher ignores directories."""
        from cc.tools.live_monitor import SessionWatcher

        callback = Mock()
        watcher = SessionWatcher(callback)

        event = Mock()
        event.is_directory = True
        event.src_path = "/path/to/dir.jsonl"

        watcher.on_modified(event)

        callback.assert_not_called()

    def test_live_monitor_parse_timestamp(self, temp_projects_dir):
        """Test timestamp parsing in LiveMonitor."""
        from cc.tools.live_monitor import LiveMonitor

        monitor = LiveMonitor(temp_projects_dir)

        ts = monitor.parse_timestamp("2025-12-10T14:30:00Z")
        assert isinstance(ts, datetime)

        # Invalid should return current time (not None)
        ts = monitor.parse_timestamp("invalid")
        assert isinstance(ts, datetime)

    def test_live_monitor_extract_project_name(self, temp_projects_dir):
        """Test project name extraction."""
        from cc.tools.live_monitor import LiveMonitor

        monitor = LiveMonitor(temp_projects_dir)

        # Test with hyphenated path
        name = monitor.extract_project_name("/path/to/project-myapp/session.jsonl")
        assert name == "myapp"

        # Test with unknown format
        name = monitor.extract_project_name("/simple/session.jsonl")
        assert name == "unknown" or len(name) <= 20

    def test_live_monitor_stats_tracking(self, temp_projects_dir):
        """Test stats are properly initialized."""
        from cc.tools.live_monitor import LiveMonitor

        monitor = LiveMonitor(temp_projects_dir)

        assert 'sessions_active' in monitor.stats
        assert 'total_messages' in monitor.stats
        assert 'total_tools' in monitor.stats
        assert 'errors' in monitor.stats

    def test_live_monitor_process_user_entry(self, temp_projects_dir):
        """Test processing user message entry."""
        from cc.tools.live_monitor import LiveMonitor

        monitor = LiveMonitor(temp_projects_dir)

        entry = {
            "type": "user",
            "sessionId": "test-123",
            "timestamp": "2025-12-10T14:30:00Z",
            "message": {
                "role": "user",
                "content": "Hello world"
            }
        }

        monitor.process_entry("/path/to/project-test/session.jsonl", entry)

        assert monitor.stats['total_messages'] == 1
        assert "test-123" in monitor.stats['sessions_active']

    def test_live_monitor_process_tool_use_entry(self, temp_projects_dir):
        """Test processing assistant entry with tool use."""
        from cc.tools.live_monitor import LiveMonitor

        monitor = LiveMonitor(temp_projects_dir)

        entry = {
            "type": "assistant",
            "sessionId": "test-123",
            "timestamp": "2025-12-10T14:30:00Z",
            "message": {
                "role": "assistant",
                "content": [
                    {
                        "type": "tool_use",
                        "name": "Read",
                        "input": {"file_path": "/path/to/file.py"}
                    }
                ]
            }
        }

        monitor.process_entry("/path/to/project-test/session.jsonl", entry)

        assert monitor.stats['total_tools'] == 1


# ============================================================================
# Tests for CC Analytics launcher
# ============================================================================

class TestCCAnalyticsLauncher:
    """Tests for the unified launcher."""

    def test_banner_defined(self):
        """Test that BANNER is defined."""
        from cc.tools.cc_analytics import BANNER

        assert "Analytics" in BANNER or "ANALYTICS" in BANNER

    def test_help_defined(self):
        """Test that HELP is defined."""
        from cc.tools.cc_analytics import HELP

        assert "report" in HELP
        assert "dashboard" in HELP
        assert "monitor" in HELP
        assert "coach" in HELP


# ============================================================================
# Integration tests
# ============================================================================

class TestIntegration:
    """Integration tests for the analytics suite."""

    def test_full_analysis_pipeline(self, temp_projects_dir, session_file):
        """Test full analysis pipeline."""
        from cc.tools.conversation_analyzer import ConversationAnalyzer

        # Create a second session file
        project_dir = temp_projects_dir / "project-api"
        project_dir.mkdir(exist_ok=True)
        session2 = project_dir / "session-456.jsonl"

        entries = [
            {
                "type": "user",
                "sessionId": "session-456",
                "timestamp": "2025-12-10T15:00:00Z",
                "message": {"role": "user", "content": "Build an API"}
            },
            {
                "type": "assistant",
                "sessionId": "session-456",
                "timestamp": "2025-12-10T15:00:05Z",
                "message": {
                    "role": "assistant",
                    "content": [
                        {"type": "tool_use", "name": "Write", "input": {"file_path": "/api/main.py", "content": "..."}}
                    ]
                }
            }
        ]

        with open(session2, 'w') as f:
            for entry in entries:
                f.write(json.dumps(entry) + '\n')

        # Analyze project 1
        analyzer1 = ConversationAnalyzer(temp_projects_dir / "project-myapp")
        analyzer1.analyze_all()

        # Analyze project 2
        analyzer2 = ConversationAnalyzer(temp_projects_dir / "project-api")
        analyzer2.analyze_all()

        # Both should have sessions
        assert len(analyzer1.sessions) >= 1
        assert len(analyzer2.sessions) >= 1

    def test_different_analyzers_same_data(self, temp_projects_dir, session_file):
        """Test that different analyzers produce consistent results."""
        from cc.tools.conversation_analyzer import ConversationAnalyzer
        from cc.tools.dashboard import DashboardAnalyzer
        from cc.tools.productivity_coach import ProductivityCoach

        # All three should be able to analyze the same data
        conv_analyzer = ConversationAnalyzer(temp_projects_dir / "project-myapp")
        conv_analyzer.analyze_all()

        dash_analyzer = DashboardAnalyzer(temp_projects_dir)
        dash_data = dash_analyzer.scan_sessions()

        coach = ProductivityCoach(temp_projects_dir)
        coach.analyze_sessions()

        # All should find at least one session
        assert len(conv_analyzer.sessions) >= 1
        assert dash_data.total_sessions >= 1
        assert coach.total_sessions >= 1

    def test_malformed_json_handling(self, temp_projects_dir):
        """Test handling of malformed JSON in session files."""
        from cc.tools.conversation_analyzer import ConversationAnalyzer

        project_dir = temp_projects_dir / "project-broken"
        project_dir.mkdir()

        broken_file = project_dir / "broken.jsonl"
        with open(broken_file, 'w') as f:
            f.write("not valid json\n")
            f.write('{"type": "user", "message": {"content": "valid"}}\n')
            f.write("{broken json here\n")

        analyzer = ConversationAnalyzer(project_dir)
        # Should not crash
        stats = analyzer.analyze_session(broken_file)

        # Should still parse the valid line
        assert stats is not None or stats is None  # Either is acceptable


# ============================================================================
# Run tests
# ============================================================================

if __name__ == "__main__":
    pytest.main([__file__, "-v"])
